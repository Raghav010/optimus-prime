{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining all the transformer classes\n",
    "\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    # enc_dev signifies if its a encoder-decoder attention head\n",
    "    def __init__(self,input_size,query_size,value_size,self_regress=False,enc_dec=False):\n",
    "        super().__init__()\n",
    "        self.wq=nn.Linear(input_size,query_size,bias=False) # W_q matrix\n",
    "        self.wk=nn.Linear(input_size,query_size,bias=False) # W_k matrix\n",
    "        self.wv=nn.Linear(input_size,value_size,bias=False) # W_v matrix\n",
    "        self.ec=enc_dec # indicates whether this attention head is doing encoder-decoder attention\n",
    "        self.self_regress=self_regress\n",
    "    \n",
    "\n",
    "    # computes the final vectors of each token\n",
    "    # N -> Batch Size\n",
    "    # L -> Sequence Lengtj\n",
    "    # Q -> (N,L,eq)\n",
    "    # K -> (N,L,ek)\n",
    "    # V -> (N,L,ev)\n",
    "    # mask -> (N,L,L)\n",
    "    # out -> (N,L,ev)\n",
    "    def SelfAttention(self,Q,K,V,mask):\n",
    "        key_size=K.shape[-1]\n",
    "        out=torch.matmul(Q,torch.transpose(K,1,2))\n",
    "        # out=torch.div(out,math.sqrt(key_size))\n",
    "        sft=nn.Softmax(dim=2)\n",
    "        attention_weights=sft(torch.div(torch.add(out,mask),math.sqrt(key_size)))\n",
    "        out=torch.matmul(attention_weights,V)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # padding mask given in the form of [0s and 1s] 0-pay attention 1-donot pay attention\n",
    "    # padding mask -> (N,L)\n",
    "    # input -> (N,L,input_size)\n",
    "    # self_regress: Boolean\n",
    "    def forward(self,input,padding_mask,K_inp=None,V_inp=None):\n",
    "\n",
    "        if not self.ec:\n",
    "            K_inp=input\n",
    "            V_inp=input\n",
    "        # calculating the Q,K,V matrices\n",
    "        Q=self.wq(input)\n",
    "        K=self.wk(K_inp)\n",
    "        V=self.wv(V_inp)\n",
    "        \n",
    "        # making the attention mask\n",
    "        batch_size=input.shape[0]\n",
    "        seqlen=input.shape[1]\n",
    "        mask=torch.unsqueeze(padding_mask,1).repeat(1,input.shape[1],1)*float('-inf') # padding mask\n",
    "        mask=torch.nan_to_num(mask,nan=0,neginf=float('-inf'))\n",
    "        if self.self_regress:\n",
    "            # self-regress mask\n",
    "            selfRegressMask=torch.triu(torch.ones(batch_size,seqlen, seqlen) * float('-inf'), diagonal=1)\n",
    "            mask=torch.add(mask,selfRegressMask)\n",
    "\n",
    "        # computing self attention\n",
    "        out=self.SelfAttention(Q,K,V,mask)\n",
    "        return out,Q,K,V\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_HeadAttention(nn.Module):\n",
    "    # enc_dev signifies if its a encoder-decoder multi-head attention\n",
    "    def __init__(self,head_count,input_size,query_size,value_size,self_regress=False,enc_dec=False):\n",
    "        super().__init__()\n",
    "        self.finLinear=nn.Linear(head_count*value_size,value_size)\n",
    "        self.ec=enc_dec\n",
    "        self.heads=[]\n",
    "        for h in head_count:\n",
    "            self.heads.append(AttentionHead(input_size,query_size,value_size,self_regress,enc_dec))\n",
    "    \n",
    "\n",
    "\n",
    "    # padding mask given in the form of [0s and 1s] 0-pay attention 1-donot pay attention\n",
    "    # padding mask -> (N,L)\n",
    "    # input -> (N,L,input_size)\n",
    "    # self_regress: Boolean\n",
    "    # returns ((N,L,ev),list of ks,list of vs)\n",
    "    def forward(self,input,padding_mask,K_inp=None,V_inp=None):\n",
    "        out_matrices=[]\n",
    "        # if return_k_v:\n",
    "        #     ks=[]\n",
    "        #     vs=[]\n",
    "        for head_id,head in enumerate(self.heads):\n",
    "            headout=head(input,padding_mask,K_inp,V_inp) \n",
    "            out_matrices.append(headout[0])\n",
    "        \n",
    "        # concatenating and feeding through linear layer\n",
    "        mh_out=self.finLinear(torch.cat(tuple(out_matrices),dim=2))\n",
    "\n",
    "        return mh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one encoder block\n",
    "# take care of passing ks and vs to decoder\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,input_size,head_count):\n",
    "        super().__init__()\n",
    "        self.LN=nn.LayerNorm(input_size)\n",
    "        self.feedForward=nn.Linear(input_size,input_size)\n",
    "        self.multiHAttention=Multi_HeadAttention(head_count,input_size,input_size,input_size)\n",
    "\n",
    "    # inputs -> (N,L,input_size) , these have to be positional encodings\n",
    "    # padding mask -> (N,L)\n",
    "    def forward(self,inputs,padding_mask):\n",
    "        out1=self.multiHAttention(inputs,padding_mask)\n",
    "        out1=self.LN(torch.add(inputs,out1))\n",
    "        out=self.feedForward(out1)\n",
    "        out=self.LN(torch.add(out1,out))\n",
    "        return out,padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,input_size,head_count):\n",
    "        super().__init__()\n",
    "        self.LN=nn.LayerNorm(input_size)\n",
    "        self.feedForward=nn.Linear(input_size,input_size)\n",
    "        self.multiHAttention=Multi_HeadAttention(head_count,input_size,input_size,input_size,self_regress=True)\n",
    "        self.encdecAttention=Multi_HeadAttention(head_count,input_size,input_size,input_size,enc_dec=True)\n",
    "\n",
    "    # inputs -> (N,L,input_size) , these have to be positional encodings\n",
    "    # padding_mask_enc -> padding mask of encoder, needed in encoder decoder attention\n",
    "    # padding mask -> (N,L)\n",
    "    # K_inp,V_inp -> (N,L,input_size)\n",
    "    def forward(self,inputs,padding_mask,K_inp,V_inp,padding_mask_enc):\n",
    "        out1=self.multiHAttention(inputs,padding_mask)\n",
    "        out1=self.LN(torch.add(inputs,out1))\n",
    "        out2=self.encdecAttention(out1,padding_mask_enc,K_inp=K_inp,V_inp=V_inp)\n",
    "        out2=self.LN(torch.add(out1,out2))\n",
    "        out=self.feedForward(out2)\n",
    "        out=self.LN(torch.add(out2,out))\n",
    "        return out,padding_mask,K_inp,V_inp,padding_mask_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self,layers,input_size,head_count):\n",
    "        super().__init__()\n",
    "        # using sequential\n",
    "        encoderStack=nn.Sequential()\n",
    "        for i in range(layers):\n",
    "            encoderStack.append(EncoderBlock(input_size,head_count))\n",
    "        self.es=encoderStack\n",
    "    \n",
    "    # inputs -> (N,L,input_size) , these have to be positional encodings\n",
    "    # padding mask -> (N,L)\n",
    "    def forward(self,inputs,padding_mask):\n",
    "        out=self.es(inputs,padding_mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self,layers,input_size,head_count):\n",
    "        super().__init__()\n",
    "        # using sequential\n",
    "        decoderStack=nn.Sequential()\n",
    "        for i in range(layers):\n",
    "            decoderStack.append(DecoderBlock(input_size,head_count))\n",
    "        self.ds=decoderStack\n",
    "    \n",
    "    # inputs -> (N,L,input_size) , these have to be positional encodings\n",
    "    # padding mask -> (N,L)\n",
    "    # padding_mask_enc -> (N,L)\n",
    "    # enc_outputs -> (N,L,input_size)\n",
    "    def forward(self,inputs,padding_mask,enc_outputs,padding_mask_enc):\n",
    "        out=self.ds(inputs,padding_mask,enc_outputs,enc_outputs,padding_mask_enc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_custom(nn.Module):\n",
    "    def __init__(self,layers,embedding_size,head_count,inp_vocab_size,out_vocab_size):\n",
    "        super().__init__()\n",
    "        # embedding layer for both encoder and decoder\n",
    "        self.embeddingsEnc=nn.Embedding(inp_vocab_size,embedding_size,0) # pad token is at index 0\n",
    "        self.embeddingsDec=nn.Embedding(out_vocab_size,embedding_size,0)\n",
    "        # positional embedding layer\n",
    "        # encoder layer\n",
    "        self.encoder=EncoderStack(layers,embedding_size,head_count)\n",
    "        # decoder layer\n",
    "        self.decoder=DecoderStack(layers,embedding_size,head_count)\n",
    "        self.toVocab=nn.Linear(embedding_size,out_vocab_size)\n",
    "        self.sft=nn.Softmax(dim=2)\n",
    "\n",
    "    # inputs,outputs -> (N,L,input_size)\n",
    "    # inp_padding,out_padding -> (N,L)\n",
    "    # returns out -> (N,L,out_vocab_size)\n",
    "    def forward(self,inputs,inp_padding,outputs,out_padding):\n",
    "        enc_embeddings=self.embeddingsEnc(inputs)\n",
    "        # add positional embedding\n",
    "        enc_outputs=self.encoder(enc_embeddings,inp_padding)\n",
    "        \n",
    "        dec_embeddings=self.embeddingsDec(outputs)\n",
    "        # add positional embeddings\n",
    "        out=self.decoder(dec_embeddings,out_padding,enc_outputs[0],enc_outputs[1])\n",
    "        out=self.toVocab(out)\n",
    "        out=self.sft(out)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are input_size==query_size==value_size???\n",
    "# add activations after linear layers\n",
    "# add multiple layer norms\n",
    "# add mask to encoder states to in enc-decoder side?? two masks? self-regress? -- got it\n",
    "# keys and value matrices how propagated into decoder?? -- got it\n",
    "# is the encoder decoder attention self regressed -- got it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# testing-------------------------------\n",
    "# N -> Batch Size\n",
    "    # L -> Sequence Lengtj\n",
    "    # Q -> (N,L,eq)\n",
    "    # K -> (N,L,ek)\n",
    "    # V -> (N,L,ev)\n",
    "    # mask -> (N,L,L)\n",
    "    # out -> (N,L,ev)\n",
    "\n",
    "# a=torch.tensor([[0,0,0,0,1,1],\n",
    "#                 [0,0,0,1,1,1]],dtype=torch.float32)\n",
    "# c=torch.tensor([[0,0,1,1,1,1],\n",
    "#                 [0,0,0,1,1,1]],dtype=torch.float32)\n",
    "# a=torch.unsqueeze(a,1)\n",
    "# c=torch.unsqueeze(c,1)\n",
    "# print(a.shape)\n",
    "# b=torch.nan_to_num(a.repeat(1,4,1)*float('-inf'),nan=0,neginf=float('-inf'))\n",
    "# d=torch.nan_to_num(c.repeat(1,4,1)*float('-inf'),nan=0,neginf=float('-inf'))\n",
    "# f=torch.add(b,d)\n",
    "# print(f)\n",
    "# sft=nn.Softmax(dim=2)\n",
    "# print(sft(f))\n",
    "# a=[1,2,3]\n",
    "# for k,v in enumerate(a):\n",
    "#     print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
