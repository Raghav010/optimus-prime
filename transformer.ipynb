{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.fr import French\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining all the transformer classes\n",
    "\n",
    "\n",
    "\n",
    "# class AttentionHead(nn.Module):\n",
    "#     # enc_dev signifies if its a encoder-decoder attention head\n",
    "#     def __init__(self,input_size,query_size,value_size,self_regress=False,enc_dec=False):\n",
    "#         super().__init__()\n",
    "#         self.wq=nn.Linear(input_size,query_size,bias=False) # W_q matrix\n",
    "#         self.wk=nn.Linear(input_size,query_size,bias=False) # W_k matrix\n",
    "#         self.wv=nn.Linear(input_size,value_size,bias=False) # W_v matrix\n",
    "#         self.ec=enc_dec # indicates whether this attention head is doing encoder-decoder attention\n",
    "#         self.self_regress=self_regress\n",
    "    \n",
    "\n",
    "#     # computes the final vectors of each token\n",
    "#     # N -> Batch Size\n",
    "#     # L -> Sequence Lengtj\n",
    "#     # Q -> (N,L,eq)\n",
    "#     # K -> (N,L,ek)\n",
    "#     # V -> (N,L,ev)\n",
    "#     # mask -> (N,L,L)\n",
    "#     # out -> (N,L,ev)\n",
    "#     def SelfAttention(self,Q,K,V,mask):\n",
    "#         key_size=K.shape[-1]\n",
    "#         out=torch.matmul(Q,torch.transpose(K,1,2))\n",
    "#         # out=torch.div(out,math.sqrt(key_size))\n",
    "#         sft=nn.Softmax(dim=2)\n",
    "#         attention_weights=sft(torch.div(torch.add(out,mask),math.sqrt(key_size)))\n",
    "#         out=torch.matmul(attention_weights,V)\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # padding mask given in the form of [0s and 1s] 0-pay attention 1-donot pay attention\n",
    "#     # padding mask -> (N,L)\n",
    "#     # input -> (N,L,input_size)\n",
    "#     # self_regress: Boolean\n",
    "#     def forward(self,input,padding_mask,K_inp=None,V_inp=None):\n",
    "\n",
    "#         if not self.ec:\n",
    "#             K_inp=input\n",
    "#             V_inp=input\n",
    "#         # calculating the Q,K,V matrices\n",
    "#         Q=self.wq(input)\n",
    "#         K=self.wk(K_inp)\n",
    "#         V=self.wv(V_inp)\n",
    "        \n",
    "#         # making the attention mask\n",
    "#         batch_size=input.shape[0]\n",
    "#         seqlen=input.shape[1]\n",
    "#         mask=torch.unsqueeze(padding_mask,1).repeat(1,input.shape[1],1)*float('-inf') # padding mask\n",
    "#         mask=torch.nan_to_num(mask,nan=0,neginf=float('-inf'))\n",
    "#         if self.self_regress:\n",
    "#             # self-regress mask\n",
    "#             selfRegressMask=torch.triu(torch.ones(batch_size,seqlen, seqlen) * float('-inf'), diagonal=1)\n",
    "#             mask=torch.add(mask,selfRegressMask)\n",
    "\n",
    "#         # computing self attention\n",
    "#         out=self.SelfAttention(Q,K,V,mask)\n",
    "#         return out,Q,K,V\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Multi_HeadAttention(nn.Module):\n",
    "#     # enc_dev signifies if its a encoder-decoder multi-head attention\n",
    "#     def __init__(self,head_count,input_size,query_size,value_size,self_regress=False,enc_dec=False):\n",
    "#         super().__init__()\n",
    "#         self.finLinear=nn.Linear(head_count*value_size,value_size)\n",
    "#         self.ec=enc_dec\n",
    "#         self.heads=[]\n",
    "#         for h in head_count:\n",
    "#             self.heads.append(AttentionHead(input_size,query_size,value_size,self_regress,enc_dec))\n",
    "    \n",
    "\n",
    "\n",
    "#     # padding mask given in the form of [0s and 1s] 0-pay attention 1-donot pay attention\n",
    "#     # padding mask -> (N,L)\n",
    "#     # input -> (N,L,input_size)\n",
    "#     # self_regress: Boolean\n",
    "#     # returns ((N,L,ev),list of ks,list of vs)\n",
    "#     def forward(self,input,padding_mask,K_inp=None,V_inp=None):\n",
    "#         out_matrices=[]\n",
    "#         # if return_k_v:\n",
    "#         #     ks=[]\n",
    "#         #     vs=[]\n",
    "#         for head_id,head in enumerate(self.heads):\n",
    "#             headout=head(input,padding_mask,K_inp,V_inp) \n",
    "#             out_matrices.append(headout[0])\n",
    "        \n",
    "#         # concatenating and feeding through linear layer\n",
    "#         mh_out=self.finLinear(torch.cat(tuple(out_matrices),dim=2))\n",
    "\n",
    "#         return mh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_HeadAttention(nn.Module):\n",
    "    # enc_dev signifies if its a encoder-decoder multi-head attention\n",
    "    def __init__(self,head_count,input_size,query_size,value_size,self_regress=False,enc_dec=False):\n",
    "        super().__init__()\n",
    "        self.ec=enc_dec\n",
    "        self.input_size=input_size\n",
    "        self.query_size=query_size\n",
    "        self.value_size=value_size\n",
    "        self.head_count=head_count\n",
    "        self.self_regress=self_regress\n",
    "\n",
    "        # calculate all Qs,Ks, and Vs for all heads at once\n",
    "        # bias=False?\n",
    "        self.wq=nn.Linear(input_size,head_count*query_size) # W_q matrices for all heads\n",
    "        self.wk=nn.Linear(input_size,head_count*query_size) # W_k matrices for all heads\n",
    "        self.wv=nn.Linear(input_size,head_count*value_size) # W_v matrices for all heads\n",
    "        self.finLinear=nn.Linear(head_count*value_size,input_size)\n",
    "    \n",
    "\n",
    "\n",
    "    # computes the final vectors of each token\n",
    "    # N -> Batch Size\n",
    "    # L -> Sequence Length\n",
    "    # H -> head count\n",
    "    # Q -> (N,H,L,eq)\n",
    "    # K -> (N,H,L,ek)\n",
    "    # V -> (N,H,L,ev)\n",
    "    # mask -> (N,L,L)\n",
    "    # out -> (N,H,L,ev)\n",
    "    def SelfAttention(self,Q,K,V,mask):\n",
    "        key_size=self.query_size\n",
    "        out=torch.matmul(Q,torch.transpose(K,2,3))\n",
    "        # out=torch.div(out,math.sqrt(key_size))\n",
    "        sft=nn.Softmax(dim=3)\n",
    "        mask=torch.unsqueeze(mask,1)\n",
    "        attention_weights=sft(torch.div(torch.add(out,mask),math.sqrt(key_size)))\n",
    "        out=torch.matmul(attention_weights,V)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    # padding mask given in the form of [0s and 1s] 0-pay attention 1-donot pay attention\n",
    "    # padding mask -> (N,L) , pass the encoding padding mask if its encoder-decoder attention\n",
    "    # input -> (N,L,input_size)\n",
    "    # self_regress: Boolean\n",
    "    # returns (N,L,input_size)\n",
    "    def forward(self,input,padding_mask,K_inp=None,V_inp=None):\n",
    "        batchSize=input.shape[0]\n",
    "        seqLen=input.shape[1]\n",
    "        if not self.ec:\n",
    "            K_inp=input\n",
    "            V_inp=input\n",
    "\n",
    "        \n",
    "        # calculating the Q,K,V matrices for all the heads\n",
    "        Q=self.wq(input).view(batchSize,seqLen,self.head_count,self.query_size) # (N,seqLen,headCount,query_size)\n",
    "        Q=torch.transpose(Q,1,2) # after transpose, of shape, (N,head_count,seqLen,query_size)\n",
    "        K=self.wk(K_inp).view(batchSize,K_inp.shape[1],self.head_count,self.query_size)\n",
    "        K=torch.transpose(K,1,2)\n",
    "        V=self.wv(V_inp).view(batchSize,K_inp.shape[1],self.head_count,self.value_size)\n",
    "        V=torch.transpose(V,1,2)\n",
    "\n",
    "        # generating a mask( maybe do this in higher classes )\n",
    "        mask=torch.unsqueeze(padding_mask,1).repeat(1,input.shape[1],1)*float('-inf') # padding mask\n",
    "        mask=torch.nan_to_num(mask,nan=0,neginf=float('-inf'))\n",
    "        if self.self_regress:\n",
    "            # self-regress mask\n",
    "            selfRegressMask=torch.triu(torch.ones(batchSize,seqLen, seqLen) * float('-inf'), diagonal=1).to('cuda')\n",
    "            mask=torch.add(mask,selfRegressMask)\n",
    "\n",
    "        \n",
    "        # converting into (N,L,head_count*value_size)\n",
    "        out=self.SelfAttention(Q,K,V,mask)\n",
    "        out=torch.transpose(out,1,2).contiguous().view(batchSize,seqLen,self.input_size)\n",
    "        mh_out=self.finLinear(out)\n",
    "\n",
    "\n",
    "        return mh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one encoder block\n",
    "# take care of passing ks and vs to decoder\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,input_size,head_count):\n",
    "        super().__init__()\n",
    "        self.LN1=nn.LayerNorm(input_size)\n",
    "        self.LN2=nn.LayerNorm(input_size)\n",
    "        self.feedForward=nn.Sequential(\n",
    "            nn.Linear(input_size,input_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        value_query_size=int(input_size/head_count)\n",
    "        self.multiHAttention=Multi_HeadAttention(head_count,input_size,value_query_size,value_query_size)\n",
    "\n",
    "    # inputs -> (N,L,input_size) , these have to be positional encodings\n",
    "    # padding mask -> (N,L)\n",
    "    def forward(self,enc_inputs):\n",
    "        (inputs,padding_mask)=enc_inputs\n",
    "\n",
    "        out1=self.multiHAttention(inputs,padding_mask)\n",
    "        out1=self.LN1(torch.add(inputs,out1))\n",
    "        out=self.feedForward(out1)\n",
    "        out=self.LN2(torch.add(out1,out))\n",
    "        return (out,padding_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,input_size,head_count):\n",
    "        super().__init__()\n",
    "        self.LN1=nn.LayerNorm(input_size)\n",
    "        self.LN2=nn.LayerNorm(input_size)\n",
    "        self.LN3=nn.LayerNorm(input_size)\n",
    "        self.feedForward=nn.Sequential(\n",
    "            nn.Linear(input_size,input_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        value_query_size=int(input_size/head_count)\n",
    "        self.multiHAttention=Multi_HeadAttention(head_count,input_size,value_query_size,value_query_size,self_regress=True)\n",
    "        self.encdecAttention=Multi_HeadAttention(head_count,input_size,value_query_size,value_query_size,enc_dec=True)\n",
    "\n",
    "    # inputs -> (N,L,input_size) , these have to be positional encodings\n",
    "    # padding_mask_enc -> padding mask of encoder, needed in encoder decoder attention\n",
    "    # padding mask -> (N,L)\n",
    "    # K_inp,V_inp -> (N,L,input_size)\n",
    "    def forward(self,dec_inputs):\n",
    "        (inputs,padding_mask,K_inp,V_inp,padding_mask_enc)=dec_inputs\n",
    "\n",
    "\n",
    "        out1=self.multiHAttention(inputs,padding_mask)\n",
    "        out1=self.LN1(torch.add(inputs,out1))\n",
    "        out2=self.encdecAttention(out1,padding_mask_enc,K_inp=K_inp,V_inp=V_inp)\n",
    "        out2=self.LN2(torch.add(out1,out2))\n",
    "        out=self.feedForward(out2)\n",
    "        out=self.LN3(torch.add(out2,out))\n",
    "        return (out,padding_mask,K_inp,V_inp,padding_mask_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self,layers,input_size,head_count):\n",
    "        super().__init__()\n",
    "        # using sequential\n",
    "        encoderStack=nn.Sequential()\n",
    "        for i in range(layers):\n",
    "            encoderStack.append(EncoderBlock(input_size,head_count))\n",
    "        self.es=encoderStack\n",
    "    \n",
    "    # inputs -> (N,L,input_size) , these have to be positional encodings\n",
    "    # padding mask -> (N,L)\n",
    "    def forward(self,inputs,padding_mask):\n",
    "        out=self.es((inputs,padding_mask))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self,layers,input_size,head_count):\n",
    "        super().__init__()\n",
    "        # using sequential\n",
    "        decoderStack=nn.Sequential()\n",
    "        for i in range(layers):\n",
    "            decoderStack.append(DecoderBlock(input_size,head_count))\n",
    "        self.ds=decoderStack\n",
    "    \n",
    "    # inputs -> (N,L,input_size) , these have to be positional encodings\n",
    "    # padding mask -> (N,L)\n",
    "    # padding_mask_enc -> (N,L)\n",
    "    # enc_outputs -> (N,L,input_size)\n",
    "    def forward(self,inputs,padding_mask,enc_outputs,padding_mask_enc):\n",
    "        out=self.ds((inputs,padding_mask,enc_outputs,enc_outputs,padding_mask_enc))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_custom(nn.Module):\n",
    "    def __init__(self,layers,embedding_size,head_count,inp_vocab_size,out_vocab_size):\n",
    "        super().__init__()\n",
    "        # embedding layer for both encoder and decoder\n",
    "        self.embeddingsEnc=nn.Embedding(inp_vocab_size,embedding_size,0) # pad token is at index 0\n",
    "        self.embeddingsDec=nn.Embedding(out_vocab_size,embedding_size,0)\n",
    "        # positional embedding layer\n",
    "        # encoder layer\n",
    "        self.encoder=EncoderStack(layers,embedding_size,head_count)\n",
    "        # decoder layer\n",
    "        self.decoder=DecoderStack(layers,embedding_size,head_count)\n",
    "        self.toVocab=nn.Linear(embedding_size,out_vocab_size)\n",
    "        # self.sft=nn.Softmax(dim=2)\n",
    "\n",
    "    # inputs,outputs -> (N,L,input_size)\n",
    "    # inp_padding,out_padding -> (N,L)\n",
    "    # returns out -> (N,L,out_vocab_size)\n",
    "    def forward(self,inputs,inp_padding,outputs,out_padding):\n",
    "        enc_embeddings=self.embeddingsEnc(inputs)\n",
    "        # add positional embedding\n",
    "        enc_outputs=self.encoder(enc_embeddings,inp_padding)\n",
    "        \n",
    "        dec_embeddings=self.embeddingsDec(outputs)\n",
    "        # add positional embeddings\n",
    "        out=self.decoder(dec_embeddings,out_padding,enc_outputs[0],enc_outputs[1])\n",
    "        out=self.toVocab(out[0])\n",
    "        # out=self.sft(out)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sentences 30000\n",
      "Val Sentences 887\n",
      "Test Sentences 1305\n"
     ]
    }
   ],
   "source": [
    "## Data Preprocessing ------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# using spacy to tokenize sentences into tokens, nltk was running into some caveats\n",
    "en_nlp = English()\n",
    "fr_nlp=French()\n",
    "en_tokenizer=en_nlp.tokenizer\n",
    "fr_tokenizer=fr_nlp.tokenizer\n",
    "\n",
    "\n",
    "# Pre-cleaning the text before splitting into sentences\n",
    "# This will clean a piece of text\n",
    "def clean(t):\n",
    "    # cleaning\n",
    "    t = re.sub(r'(((http|https):\\/\\/)|www\\.)([a-zA-Z0-9]+\\.){0,2}[a-zA-Z0-9]+([a-zA-Z0-9\\/#%&=\\?_\\.\\-\\+]+)', \"\", t)\n",
    "    t = re.sub(r'(@[a-zA-Z0-9_]+)', \"\", t)\n",
    "    t = re.sub(r'(#[a-zA-Z0-9_]+\\b)', \"\", t)\n",
    "    t = re.sub(r'\\d+', \"\",t)\n",
    "    t = re.sub(r'--',\" \",t)\n",
    "    # special characters\n",
    "    t = re.sub(r'[\\_\\$\\*\\^\\(\\)\\[\\]\\{\\}\\=\\+\\<\\>\",\\&\\%\\-\\—\\”\\“\\–\\\\\\.\\?\\!;]',\" \",t)\n",
    "    t=re.sub(r'\\n',\" \",t)\n",
    "    t=t.lower()\n",
    "    return t\n",
    "\n",
    "\n",
    "def formatFiles(english_file_path,french_pile_path,output_arr):\n",
    "    with open(english_file_path) as en:\n",
    "        with open(french_pile_path) as fr:\n",
    "            for en_line,fr_line in zip(en,fr):\n",
    "                output_arr.append((en_line,fr_line))\n",
    "\n",
    "# getting the test,train,val inputs,labels\n",
    "train_sentences=[]\n",
    "test_sentences=[]\n",
    "val_sentences=[]\n",
    "\n",
    "formatFiles('/home2/raghavd0/transformer/en-fr_dataset/train.en','/home2/raghavd0/transformer/en-fr_dataset/train.fr',train_sentences)\n",
    "formatFiles('/home2/raghavd0/transformer/en-fr_dataset/test.en','/home2/raghavd0/transformer/en-fr_dataset/test.fr',test_sentences)\n",
    "formatFiles('/home2/raghavd0/transformer/en-fr_dataset/dev.en','/home2/raghavd0/transformer/en-fr_dataset/dev.fr',val_sentences)\n",
    "\n",
    "\n",
    "print('Train Sentences',len(train_sentences))\n",
    "print('Val Sentences',len(val_sentences))\n",
    "print('Test Sentences',len(test_sentences))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Unknown Tokens 8494\n",
      "French Unknown Tokens 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# data -> [[(input,label)],]\n",
    "# cleans,splits into words,and replaces less frequent tokens with unknown tokens\n",
    "def cleanData(data,min_ocuurences):\n",
    "    en_vocab_count={}\n",
    "    fr_vocab_count={}\n",
    "\n",
    "    # splitting into tokens and counting occurences of words\n",
    "    # tokenized data -> [[([],[]),],]\n",
    "    en_total_tokens=0\n",
    "    tokenized_data=[]\n",
    "    for data_pack in data:\n",
    "        tokenized_data_pack=[]\n",
    "        for sample in data_pack:\n",
    "            en_tokens=[]\n",
    "            fr_tokens=[]\n",
    "\n",
    "            for en_token in en_tokenizer(clean(sample[0])):\n",
    "                en_total_tokens+=1\n",
    "                en_token=str(en_token)\n",
    "                en_tokens.append(en_token)\n",
    "                en_vocab_count[en_token]=en_vocab_count.get(en_token,0)+1\n",
    "            for fr_token in fr_tokenizer(clean(sample[1])):\n",
    "                fr_token=str(fr_token)\n",
    "                fr_tokens.append(fr_token)\n",
    "                fr_vocab_count[fr_token]=fr_vocab_count.get(fr_token,0)+1\n",
    "            \n",
    "            tokenized_data_pack.append((en_tokens,fr_tokens))\n",
    "        tokenized_data.append(tokenized_data_pack)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # replacing low occuring words in english with <UNK> token\n",
    "    en_unk=set()\n",
    "    fr_unk=set()\n",
    "    for data_pack in tokenized_data:\n",
    "        for sample in data_pack: # sample = ([],[])\n",
    "\n",
    "            for i,en_token in enumerate(sample[0]):\n",
    "                if en_vocab_count[en_token]<min_ocuurences:\n",
    "                    sample[0][i]='<UNK>'\n",
    "                    en_unk.add(en_token)\n",
    "            # for i,fr_token in enumerate(sample[1]):\n",
    "            #     if fr_vocab_count[fr_token]<min_ocuurences:\n",
    "            #         sample[1][i]='<UNK>'\n",
    "            #         fr_unk.add(fr_token)\n",
    "    \n",
    "    print('English Unknown Tokens',len(en_unk))\n",
    "    print('French Unknown Tokens',len(fr_unk))\n",
    "\n",
    "    return tokenized_data            \n",
    "\n",
    "\n",
    "cleaned_tok_data=cleanData([train_sentences,val_sentences,test_sentences],2)\n",
    "\n",
    "# 22k unique tokens in english\n",
    "# 29k unique tokens in french\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cleaned_tok_data[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13408 ['<PAD>', '<SOS>', '<EOS>', 'david', 'gallo', ':', 'this', 'is', 'bill', 'lange']\n",
      "28940 ['<PAD>', '<SOS>', '<EOS>', 'david', 'gallo', ':', 'voici', 'bill', 'lange', ' ']\n"
     ]
    }
   ],
   "source": [
    "en_vocab=['<PAD>','<SOS>','<EOS>']\n",
    "en_wordToIdx={'<PAD>':0,'<SOS>':1,'<EOS>':2} # word:index\n",
    "\n",
    "fr_vocab=['<PAD>','<SOS>','<EOS>']\n",
    "fr_wordToIdx={'<PAD>':0,'<SOS>':1,'<EOS>':2}\n",
    "\n",
    "# indexing all the words and constructing vocabulary\n",
    "for data_pack in cleaned_tok_data:\n",
    "    for sample in data_pack:\n",
    "        for en_token in sample[0]:\n",
    "            if en_wordToIdx.get(en_token,0)==0:\n",
    "                en_wordToIdx[en_token]=len(en_vocab)\n",
    "                en_vocab.append(en_token)\n",
    "        for fr_token in sample[1]:\n",
    "            if fr_wordToIdx.get(fr_token,0)==0:\n",
    "                fr_wordToIdx[fr_token]=len(fr_vocab)\n",
    "                fr_vocab.append(fr_token)\n",
    "\n",
    "print(len(en_vocab),en_vocab[:10])\n",
    "print(len(fr_vocab),fr_vocab[:10])\n",
    "en_vocab_len=len(en_vocab)\n",
    "fr_vocab_len=len(fr_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokToIdx(wordToIdx,tokens,add_sos=False,add_eos=False):\n",
    "    indexes=[]\n",
    "    if add_sos:\n",
    "        indexes.append(wordToIdx['<SOS>'])\n",
    "    for tok in tokens:\n",
    "        indexes.append(wordToIdx[tok])\n",
    "    if add_eos:\n",
    "        indexes.append(wordToIdx['<EOS>'])\n",
    "    return indexes\n",
    "\n",
    "def IdxToTok(vocab,tokens):\n",
    "    toks=[]\n",
    "    for idx in tokens:\n",
    "        toks.append(vocab[idx])\n",
    "    return toks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# converting tokens to indices\n",
    "indexed_data=[]\n",
    "\n",
    "for data_pack in cleaned_tok_data:\n",
    "    indexed_data_pack=[]\n",
    "    for sample in data_pack:\n",
    "        indexed_en=tokToIdx(en_wordToIdx,sample[0])\n",
    "        indexed_fr=tokToIdx(fr_wordToIdx,sample[1],True,True)\n",
    "        indexed_data_pack.append((indexed_en,indexed_fr))\n",
    "    indexed_data.append(indexed_data_pack)\n",
    "\n",
    "# print(indexed_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_en_len=-1\n",
    "# max_fr_len=-1\n",
    "# for datapk in indexed_data:\n",
    "#     for sample in datapk:\n",
    "#         max_en_len=max(len(sample[0]),max_en_len)\n",
    "#         max_fr_len=max(len(sample[1]),max_fr_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725 733\n"
     ]
    }
   ],
   "source": [
    "# print(max_en_len,max_fr_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "# making dataset and dataloader-----------------------------------\n",
    "\n",
    "\n",
    "class en_fr_Dataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # returns input_sequence,decoder_input_sequence,decoder_target\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.data[idx][0],self.data[idx][1][:-1].copy(),self.data[idx][1][1:])\n",
    "\n",
    "\n",
    "# returns (enc_inputs,enc_inputs_mask,dec_inputs,dec_inputs_mask,dec_targets)\n",
    "def collater(data):\n",
    "    enc_inputs=[]\n",
    "    dec_inputs=[]\n",
    "    dec_targets=[]\n",
    "    for m in data:\n",
    "        enc_inputs.append(torch.tensor(m[0],dtype=torch.int))\n",
    "        dec_inputs.append(torch.tensor(m[1],dtype=torch.int))\n",
    "        dec_targets.append(torch.tensor(m[2],dtype=torch.int))\n",
    "    \n",
    "\n",
    "\n",
    "    enc_inputs=pad_sequence(enc_inputs,batch_first=True)\n",
    "    dec_inputs=pad_sequence(dec_inputs,batch_first=True)\n",
    "    dec_targets=one_hot(pad_sequence(dec_targets,batch_first=True).long(),num_classes=fr_vocab_len).to(torch.float32)\n",
    "\n",
    "    enc_inputs_mask=(enc_inputs==0).int()\n",
    "    dec_inputs_mask=(dec_inputs==0).int() # 0 - pay attention, 1 - no attention\n",
    "\n",
    "    return (enc_inputs,enc_inputs_mask,dec_inputs,dec_inputs_mask,dec_targets)\n",
    "\n",
    "\n",
    "train_DL=DataLoader(en_fr_Dataset(indexed_data[0]),batch_size=3,collate_fn=collater)\n",
    "val_DL=DataLoader(en_fr_Dataset(indexed_data[1]),batch_size=3,collate_fn=collater)\n",
    "\n",
    "# count=0\n",
    "# for batch in train_DL:\n",
    "#     print('Encoder inputs',batch[0])\n",
    "#     print('Encoder inputs mask',batch[1])\n",
    "#     print('Decoder inputs',batch[2])\n",
    "#     print('Decoder inputs mask',batch[3])\n",
    "#     print('Decoder targets',batch[4])\n",
    "#     count+=1\n",
    "#     if count==3:\n",
    "#         break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "\n",
    "# Load model,optimizer and loss function and get device\n",
    "model=Transformer_custom(2,300,2,en_vocab_len,fr_vocab_len).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,DL,loss_fn,optimizer):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    count=0\n",
    "    for enc_inputs,enc_inputs_mask,dec_inputs,dec_inputs_mask,dec_targets in DL:\n",
    "\n",
    "        enc_inputs,enc_inputs_mask,dec_inputs,dec_inputs_mask,dec_targets=enc_inputs.to(device),enc_inputs_mask.to(device),dec_inputs.to(device),dec_inputs_mask.to(device),dec_targets.to(device)\n",
    "        logits=model(enc_inputs,enc_inputs_mask,dec_inputs,dec_inputs_mask)\n",
    "        \n",
    "        logits,dec_targets=torch.flatten(logits,start_dim=0,end_dim=1),torch.flatten(dec_targets,start_dim=0,end_dim=1)\n",
    "\n",
    "        # calculating loss\n",
    "        loss=loss_fn(logits,dec_targets)\n",
    "        # print(loss.item())\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        count+=1\n",
    "        with torch.no_grad():\n",
    "            print(f'Train-----Batch {count} Perplexity ----> {torch.exp(loss).item()}\\r',end=\"\")\n",
    "\n",
    "\n",
    "    perp=torch.exp(torch.tensor(total_loss/len(DL),dtype=torch.float32)).item()\n",
    "    print(f'Train Perplexity: {perp}\\n')\n",
    "    return perp\n",
    "\n",
    "# test\n",
    "def test(model,DL,loss_fn,optimizer):\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    count=0\n",
    "    with torch.no_grad():\n",
    "        for enc_inputs,enc_inputs_mask,dec_inputs,dec_inputs_mask,dec_targets in DL:\n",
    "\n",
    "            enc_inputs,enc_inputs_mask,dec_inputs,dec_inputs_mask,dec_targets=enc_inputs.to(device),enc_inputs_mask.to(device),dec_inputs.to(device),dec_inputs_mask.to(device),dec_targets.to(device)\n",
    "            logits=model(enc_inputs,enc_inputs_mask,dec_inputs,dec_inputs_mask)\n",
    "            \n",
    "            logits,dec_targets=torch.flatten(logits,start_dim=0,end_dim=1),torch.flatten(dec_targets,start_dim=0,end_dim=1)\n",
    "\n",
    "            # calculating loss\n",
    "            loss=loss_fn(logits,dec_targets)\n",
    "            # print(loss.item())\n",
    "            total_loss+=loss.item()\n",
    "\n",
    "            count+=1\n",
    "            print(f'Test-----Batch {count} Perplexity ----> {torch.exp(loss).item()}\\r',end=\"\")\n",
    "\n",
    "\n",
    "        perp=torch.exp(torch.tensor(total_loss/len(DL),dtype=torch.float32)).item()\n",
    "        print(f'Test Perplexity: {perp}\\n')\n",
    "    return perp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "headers=['EpochNumber','TrainAveragePerlplexity','ValAveragePerlplexity']\n",
    "\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "with open('Stats.csv','w') as csvh:\n",
    "    csvwriter = csv.writer(csvh, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csvwriter.writerow(headers)\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_stats=train(model,train_DL,loss_fn,optimizer)\n",
    "        val_stats=test(model,val_DL,loss_fn,optimizer)\n",
    "\n",
    "        row=[t+1]\n",
    "        row.append(train_stats)\n",
    "        row.append(val_stats)\n",
    "\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "        # saving model after 2 epochs\n",
    "        # if (t+1)%2==0:\n",
    "        #     torch.save(model,'BiLSTM'+str(t+1)+'epoch.pth')\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are input_size==query_size==value_size???\n",
    "# add activations after linear layers\n",
    "# add multiple layer norms\n",
    "# add mask to encoder states to in enc-decoder side?? two masks? self-regress? -- got it\n",
    "# keys and value matrices how propagated into decoder?? -- got it\n",
    "# is the encoder decoder attention self regressed -- got it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# testing-------------------------------\n",
    "# N -> Batch Size\n",
    "    # L -> Sequence Lengtj\n",
    "    # Q -> (N,L,eq)\n",
    "    # K -> (N,L,ek)\n",
    "    # V -> (N,L,ev)\n",
    "    # mask -> (N,L,L)\n",
    "    # out -> (N,L,ev)\n",
    "\n",
    "# a=torch.tensor([[0,0,0,0,1,1],\n",
    "#                 [0,0,0,1,1,1]],dtype=torch.float32)\n",
    "# c=torch.tensor([[0,0,1,1,1,1],\n",
    "#                 [0,0,0,1,1,1]],dtype=torch.float32)\n",
    "# a=torch.unsqueeze(a,1)\n",
    "# c=torch.unsqueeze(c,1)\n",
    "# print(a.shape)\n",
    "# b=torch.nan_to_num(a.repeat(1,4,1)*float('-inf'),nan=0,neginf=float('-inf'))\n",
    "# d=torch.nan_to_num(c.repeat(1,4,1)*float('-inf'),nan=0,neginf=float('-inf'))\n",
    "# f=torch.add(b,d)\n",
    "# print(f)\n",
    "# sft=nn.Softmax(dim=2)\n",
    "# print(sft(f))\n",
    "# a=[1,2,3]\n",
    "# for k,v in enumerate(a):\n",
    "#     print(k,v)\n",
    "# import torch\n",
    "\n",
    "# a=torch.ones(2,4,10)\n",
    "# print(a)\n",
    "# b=torch.flatten(a,start_dim=0,end_dim=1)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 10\n"
     ]
    }
   ],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# a=[torch.ones(100),torch.ones(50),torch.ones(26)]\n",
    "# b=pad_sequence(a,batch_first=True)\n",
    "# print(b)\n",
    "# print(b==0)\n",
    "# print((b==0).int())\n",
    "# b=6\n",
    "# c=10\n",
    "\n",
    "# a=(b,c)\n",
    "\n",
    "# (d,m)=a\n",
    "# print(d,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# from torch.nn.functional import one_hot\n",
    "\n",
    "# a=torch.tensor([[1,2,3,5],\n",
    "#                [2,3,5,6]])\n",
    "# print(one_hot(a).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
